var documenterSearchIndex = {"docs":
[{"location":"arches/tigerlake/#Tigerlake","page":"Tigerlake","title":"Tigerlake","text":"","category":"section"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"Tigerlake CPUs feature just a single 512-bit-fma unit, and thus their theoretical peak FLOPS are comparable with AVX2 CPUs featuing two 256-bit FMA units, such as Intel's Skylake or AMD's Zen2. The much larger register file that AVX512 provides combined with its comparatively much larger L1 and L2 caches (and no doubt helped by the large out of order buffer) make it comparatively very easy to attain near peak performance on Tigerlake.","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"Statically sized benchmarks vs StaticArrays.jl: (Image: sizedbenchmarks)","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"The SMatrix and MMatrix are the immutable and immutable matrix types from StaticArrays.jl, respectively, while StrideArray.jl and PtrArray.jl are mutable array types with optional static sizing providing by StrideArrays.jl. The benchmarks also included jmul! on base Matrix{Float64}, demonstrating the performance of StrideArrays's fully dynamic multiplication function.","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"The version of OpenBLAS used (0.3.10) didn't support Tigerlake yet. Unlike Cascadelake, where approaching the CPU's peak performance can be challenging, it is easy with Tigerlake: Tigerlake has much larger caches and reorder buffers, making it much more capable of feeding the execution units, but has half as many of them to feed as cascadelake for these workloads (1 FMA unit vs 2 FMA units).","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"Threaded results of the dynamic matmul: (Image: threadedbenchmarks)","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"Single threaded, the fully dynamic multiplication is competitive with MKL and OpenBLAS from around 2x2 to 256x256: (Image: dgemmbenchmarkssmall) Unlike the Cascadelake CPU, it was able to hold on with MKL at least through 2000x2000: (Image: dgemmbenchmarksmedium)","category":"page"},{"location":"broadcasting/#Broadcasting","page":"Broadcasting","title":"Broadcasting","text":"","category":"section"},{"location":"broadcasting/","page":"Broadcasting","title":"Broadcasting","text":"Broadcasting StrideArrays is also fast, e.g. to continue on the random number generation example from earlier, we could quickly calculate a Monte Carlo sample of means of log normally distributed random variables:","category":"page"},{"location":"broadcasting/","page":"Broadcasting","title":"Broadcasting","text":"julia> @benchmark sum(exp.(@StrideArray randn(8,10))) # StrideArrays\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     127.652 ns (0.00% GC)\n  median time:      129.033 ns (0.00% GC)\n  mean time:        129.041 ns (0.00% GC)\n  maximum time:     163.491 ns (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     888\n\njulia> @benchmark sum(exp.(@SMatrix randn(8,10))) # StaticArrays\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     678.948 ns (0.00% GC)\n  median time:      690.000 ns (0.00% GC)\n  mean time:        690.399 ns (0.00% GC)\n  maximum time:     847.484 ns (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     153","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To install StrideArrays.jl, simply:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"StrideArrays\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This library is built on ArrayInterface.jl and LoopVectorization.jl. It is still somewhat experimental, and many features such as good linear algebra support, are still missing. It aims to achieve high performance and provide flexibility, while keeping implementations simple.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Please file issues if you encounter problems or have feature requests.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To create an uninitialized StrideArray, use the constructor StrideArray{T}(undef, size_tuple), e.g.:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> StrideArray{Float64}(undef, (3,4)) |> StrideArrays.size\n(3, 4)\n\njulia> StrideArray{Float64}(undef, (StaticInt(3),4)) |> StrideArrays.size\n(Static(3), 4)\n\njulia> StrideArray{Float64}(undef, (3,StaticInt(4))) |> StrideArrays.size\n(3, Static(4))\n\njulia> StrideArray{Float64}(undef, (StaticInt(3),StaticInt(4))) |> StrideArrays.size\n(Static(3), Static(4))","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"If a size is specified by a StaticInt, then that dimension will be statically sized. Otherwise, it will by dynamically sized.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To create one filled with random elements, see the RNG section.","category":"page"},{"location":"arches/haswell/#Haswell","page":"Haswell","title":"Haswell","text":"","category":"section"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"The Haswell CPU benchmarked here is a 1.7 GHz laptop CPU. It features two 256-bit FMA units, which gives it comparable peak FLOPS/cycle to Tigerlake. But, with its smaller caches, fewer and smaller registers necessitating churning over the cache more quickly, and more limited out of order capabilities, it is much more difficult to achieve peak performance on Haswell.","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"Statically sized benchmarks vs StaticArrays.jl: (Image: sizedbenchmarks)","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"The SMatrix and MMatrix are the immutable and immutable matrix types from StaticArrays.jl, respectively, while StrideArray.jl and PtrArray.jl are mutable array types with optional static sizing providing by StrideArrays.jl. The benchmarks also included jmul! on base Matrix{Float64}, demonstrating the performance of StrideArrays's fully dynamic multiplication function.","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"SMatrix were only benchmarked up to size 20x20. As their performance at larger sizes recently increased, I'll increase the size range at which I benchmark them in the future.","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"The fully dynamic multiplication is competitive with MKL and OpenBLAS from around 2x2 to 256x256: (Image: dgemmbenchmarkssmall) (Image: dgemmbenchmarksmedium)","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"Benchmarks will be added later.","category":"page"},{"location":"arches/cascadelake/#Cascadelake","page":"Cascadelake","title":"Cascadelake","text":"","category":"section"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"Cascadelake CPUs feature 2 512-bit-fma units, and thus can achieve high FLOPS in BLAS-like operations. The particular CPU on which these benchmarks were run had its heavy-AVX512 clock speed set to 4.1 GHz, providing a theoretical peak of 131.2 GFLOPS/core.","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"Statically sized benchmarks vs StaticArrays.jl: (Image: sizedbenchmarks)","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"The SMatrix and MMatrix are the immutable and immutable matrix types from StaticArrays.jl, respectively, while StrideArray.jl and PtrArray.jl are mutable array types with optional static sizing providing by StrideArrays.jl. The benchmarks also included jmul! on base Matrix{Float64}, demonstrating the performance of StrideArrays's fully dynamic multiplication function.","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"SMatrix were only benchmarked up to size 20x20. As their performance at larger sizes recently increased, I'll increase the size range at which I benchmark them in the future.","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"Multithreading benchmarks were run using BLASBenchmarks.jl: (Image: multithreadedbenchmarks)","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"The single-threaded dynamic multiplication is competitive with MKL and OpenBLAS from around 2x2 to 256x256: (Image: dgemmbenchmarkssmall) However, beyond this size, performance begins to fall behind: (Image: dgemmbenchmarksmedium) OpenBLAS eventually ascends to about 120 GFLOPS, but StrideArrays seems stuck at around 100 GFLOPS.","category":"page"},{"location":"stack_allocation/#Stack-Allocation","page":"Stack Allocattion","title":"Stack Allocation","text":"","category":"section"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"Stack allocated arrays are great, as are mutable arrays.","category":"page"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"StrideArrays.jl tries it's hardest to provide you with both. As you may have noted from the RNG and broadcasting pages, we were creating mutable StrideArrays without suffering memory allocations, just like with the immutable StaticArrays.SArray type. The mutable StaticArrays.MArray, on the other hand, would have allocated:","category":"page"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"julia> @benchmark sum(exp.(@StrideArray randn(8,10))) # StrideArrays\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     127.557 ns (0.00% GC)\n  median time:      127.986 ns (0.00% GC)\n  mean time:        128.116 ns (0.00% GC)\n  maximum time:     165.890 ns (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     888\n\njulia> @benchmark sum(exp.(@MMatrix randn(8,10))) # StaticArrays\nBenchmarkTools.Trial:\n  memory estimate:  672 bytes\n  allocs estimate:  1\n  --------------\n  minimum time:     703.599 ns (0.00% GC)\n  median time:      862.130 ns (0.00% GC)\n  mean time:        887.160 ns (4.56% GC)\n  maximum time:     136.675 Î¼s (99.29% GC)\n  --------------\n  samples:          10000\n  evals/sample:     142","category":"page"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"This is achieved thanks to a convenient macro, StrideArrays.@gc_preserve. When the macro is applied to a function call, it GC.@preserves all the arrays, and substitutes them with PtrArrays. This will safely preserve the array's memory during the call, while promising that the array won't escape, so that it may be stack allocated. Otherwise, passing mutable structs to non-inlined functions currently forces heap allocation. Many functions are overloaded for StrideArrays to provide a @gc_preserve barrier, so that calling them will not force heap allocation. However, doing this systematically is still a work in progress, so please file an issue if you encounter a function commonly used on arrays, especially if already defined in StrideArrays.jl, in which this is not the case.","category":"page"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"When writing code making use of statically sized StrideArrays, you can use @gc_preserve in your own code when you can promise the array won't escape to make use of mutable stack allocated arrays. Note that @gc_preserve should also work on MArrays.","category":"page"},{"location":"#StrideArrays.jl","page":"Home","title":"StrideArrays.jl","text":"","category":"section"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n\t\"getting_started.md\",\n\t\"arches/cascadelake.md\",\n\t\"arches/tigerlake.md\",\n\t\"arches/haswell.md\",\n\t\"rng.md\",\n\t\"broadcasting.md\",\n\t\"stack_allocation.md\"\n]\nDepth = 1","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [StrideArrays]","category":"page"},{"location":"#StrideArrays.@StrideArray-Tuple{Any, Vararg{Any}}","page":"Home","title":"StrideArrays.@StrideArray","text":"@StrideArray rand(Float32, 3, 4)\n@StrideArray randn(3,  4)\n@StrideArray rand(7>>1,  4)\n\nCreates a random StrideArray. The default element type is Float64. Dimensions will be statically sized if specified by an integer literal, or if interpolated.\n\n\n\n\n\n","category":"macro"},{"location":"rng/#Random-Number-Generation","page":"Random Number Generation","title":"Random Number Generation","text":"","category":"section"},{"location":"rng/","page":"Random Number Generation","title":"Random Number Generation","text":"Randomly generating StrideArrays is fast, and can be done via a convenient macro:","category":"page"},{"location":"rng/","page":"Random Number Generation","title":"Random Number Generation","text":"julia> using StrideArrays, StaticArrays, BenchmarkTools\n\njulia> @btime sum(@StrideArray randn(8,10)) # StrideArrays\n  103.613 ns (0 allocations: 0 bytes)\n18.015335007499978\n\njulia> @btime sum(@SMatrix randn(8,10)) # StaticArrays\n  297.042 ns (0 allocations: 0 bytes)\n-4.091586809768035\n\njulia> @btime sum(@StrideArray rand(8,10)) # StrideArrays\n  18.862 ns (0 allocations: 0 bytes)\n43.61560492320911\n\njulia> @btime sum(@SMatrix rand(8,10)) # StaticArrays\n  171.001 ns (0 allocations: 0 bytes)\n38.47263930206726","category":"page"}]
}
